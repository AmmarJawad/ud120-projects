{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "import time\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "\n",
    "import missingno as msno\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features by data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### features_list selects which features to include.\n",
    "features_list = ['poi', 'salary', 'deferral_payments', 'total_payments', 'loan_advances','bonus', \n",
    "                      'restricted_stock_deferred', 'deferred_income', 'total_stock_value', \n",
    "                      'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', \n",
    "                      'restricted_stock', 'director_fees', 'to_messages', 'from_poi_to_this_person', 'from_messages', \n",
    "                       'from_this_person_to_poi', 'shared_receipt_with_poi'\n",
    "                ]\n",
    "\n",
    "# Identifying columns with financial values\n",
    "financial_features = ['salary', 'deferral_payments', 'total_payments', 'loan_advances','bonus', \n",
    "                      'restricted_stock_deferred', 'deferred_income', 'total_stock_value', \n",
    "                      'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', \n",
    "                      'restricted_stock', 'director_fees'\n",
    "                     ]\n",
    "\n",
    "# Identfying columns with numerical values\n",
    "features_with_count = ['to_messages', 'from_poi_to_this_person', 'from_messages', \n",
    "                       'from_this_person_to_poi', 'shared_receipt_with_poi'\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METTS MARK\n",
      "BAXTER JOHN C\n",
      "ELLIOTT STEVEN\n",
      "CORDES WILLIAM R\n",
      "HANNON KEVIN P\n",
      "MORDAUNT KRISTINA M\n",
      "MEYER ROCKFORD G\n",
      "MCMAHON JEFFREY\n",
      "HORTON STANLEY C\n",
      "PIPER GREGORY F\n",
      "HUMPHREY GENE E\n",
      "UMANOFF ADAM S\n",
      "BLACHMAN JEREMY M\n",
      "SUNDE MARTIN\n",
      "GIBBS DANA R\n",
      "LOWRY CHARLES P\n",
      "COLWELL WESLEY\n",
      "MULLER MARK S\n",
      "JACKSON CHARLENE R\n",
      "WESTFAHL RICHARD K\n",
      "WALTERS GARETH W\n",
      "WALLS JR ROBERT H\n",
      "KITCHEN LOUISE\n",
      "CHAN RONNIE\n",
      "BELFER ROBERT\n",
      "SHANKMAN JEFFREY A\n",
      "WODRASKA JOHN\n",
      "BERGSIEKER RICHARD P\n",
      "URQUHART JOHN A\n",
      "BIBI PHILIPPE A\n",
      "RIEKER PAULA H\n",
      "WHALEY DAVID A\n",
      "BECK SALLY W\n",
      "HAUG DAVID L\n",
      "ECHOLS JOHN B\n",
      "MENDELSOHN JOHN\n",
      "HICKERSON GARY J\n",
      "CLINE KENNETH W\n",
      "LEWIS RICHARD\n",
      "HAYES ROBERT E\n",
      "MCCARTY DANNY J\n",
      "KOPPER MICHAEL J\n",
      "LEFF DANIEL P\n",
      "LAVORATO JOHN J\n",
      "BERBERIAN DAVID\n",
      "DETMERING TIMOTHY J\n",
      "WAKEHAM JOHN\n",
      "POWERS WILLIAM\n",
      "GOLD JOSEPH\n",
      "BANNANTINE JAMES M\n",
      "DUNCAN JOHN H\n",
      "SHAPIRO RICHARD S\n",
      "SHERRIFF JOHN R\n",
      "SHELBY REX\n",
      "LEMAISTRE CHARLES\n",
      "DEFFNER JOSEPH M\n",
      "KISHKILL JOSEPH G\n",
      "WHALLEY LAWRENCE G\n",
      "MCCONNELL MICHAEL S\n",
      "PIRO JIM\n",
      "DELAINEY DAVID W\n",
      "SULLIVAN-SHAKLOVITZ COLLEEN\n",
      "WROBEL BRUCE\n",
      "LINDHOLM TOD A\n",
      "MEYER JEROME J\n",
      "LAY KENNETH L\n",
      "BUTTS ROBERT H\n",
      "OLSON CINDY K\n",
      "MCDONALD REBECCA\n",
      "CUMBERLAND MICHAEL S\n",
      "GAHN ROBERT S\n",
      "MCCLELLAN GEORGE\n",
      "HERMANN ROBERT J\n",
      "SCRIMSHAW MATTHEW\n",
      "GATHMANN WILLIAM D\n",
      "HAEDICKE MARK E\n",
      "BOWEN JR RAYMOND M\n",
      "GILLIS JOHN\n",
      "FITZGERALD JAY L\n",
      "MORAN MICHAEL P\n",
      "REDMOND BRIAN L\n",
      "BAZELIDES PHILIP J\n",
      "BELDEN TIMOTHY N\n",
      "DURAN WILLIAM D\n",
      "THORN TERENCE H\n",
      "FASTOW ANDREW S\n",
      "FOY JOE\n",
      "CALGER CHRISTOPHER F\n",
      "RICE KENNETH D\n",
      "KAMINSKI WINCENTY J\n",
      "LOCKHART EUGENE E\n",
      "COX DAVID\n",
      "OVERDYKE JR JERE C\n",
      "PEREIRA PAULO V. FERRAZ\n",
      "STABLER FRANK\n",
      "SKILLING JEFFREY K\n",
      "BLAKE JR. NORMAN P\n",
      "SHERRICK JEFFREY B\n",
      "PRENTICE JAMES\n",
      "GRAY RODNEY\n",
      "PICKERING MARK R\n",
      "THE TRAVEL AGENCY IN THE PARK\n",
      "NOLES JAMES L\n",
      "KEAN STEVEN J\n",
      "TOTAL\n",
      "FOWLER PEGGY\n",
      "WASAFF GEORGE\n",
      "WHITE JR THOMAS E\n",
      "CHRISTODOULOU DIOMEDES\n",
      "ALLEN PHILLIP K\n",
      "SHARP VICTORIA T\n",
      "JAEDICKE ROBERT\n",
      "WINOKUR JR. HERBERT S\n",
      "BROWN MICHAEL\n",
      "BADUM JAMES P\n",
      "HUGHES JAMES A\n",
      "REYNOLDS LAWRENCE\n",
      "DIMICHELE RICHARD G\n",
      "BHATNAGAR SANJAY\n",
      "CARTER REBECCA C\n",
      "BUCHANAN HAROLD G\n",
      "YEAP SOON\n",
      "MURRAY JULIA H\n",
      "GARLAND C KEVIN\n",
      "DODSON KEITH\n",
      "YEAGER F SCOTT\n",
      "HIRKO JOSEPH\n",
      "DIETRICH JANET R\n",
      "DERRICK JR. JAMES V\n",
      "FREVERT MARK A\n",
      "PAI LOU L\n",
      "BAY FRANKLIN R\n",
      "HAYSLETT RODERICK J\n",
      "FUGH JOHN L\n",
      "FALLON JAMES B\n",
      "KOENIG MARK E\n",
      "SAVAGE FRANK\n",
      "IZZO LAWRENCE L\n",
      "TILNEY ELIZABETH A\n",
      "MARTIN AMANDA K\n",
      "BUY RICHARD B\n",
      "GRAMM WENDY L\n",
      "CAUSEY RICHARD A\n",
      "TAYLOR MITCHELL S\n",
      "DONAHUE JR JEFFREY M\n",
      "GLISAN JR BEN F\n"
     ]
    }
   ],
   "source": [
    "# Ensuring that all keys refer to Enron employees\n",
    "for k, v in data_dict.iteritems():\n",
    "    print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in the dataset: 146\n"
     ]
    }
   ],
   "source": [
    "# Shape of dataset\n",
    "print \"Rows in the dataset:\", len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the 'TOTAL' value in data_dict because it is a column sum of salaries and doesn't belong to any single employee.\n",
    "del data_dict['TOTAL']\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POIs in the dataset:  18\n",
      "Non-POIs in the dataset:  126\n"
     ]
    }
   ],
   "source": [
    "# POIs and non-POIs in the dataset\n",
    "poi_num = 0\n",
    "non_poi_num = 0\n",
    "for poi in labels:\n",
    "    if poi == 1.0:\n",
    "        poi_num += 1\n",
    "    else:\n",
    "        non_poi_num += 1\n",
    "\n",
    "# Imbalanced classes of POIs - more Non-POIs than POIs.\n",
    "print \"POIs in the dataset: \", poi_num\n",
    "print \"Non-POIs in the dataset: \", non_poi_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any null-values present in the features chosen? \n",
      "0     False\n",
      "1     False\n",
      "2     False\n",
      "3     False\n",
      "4     False\n",
      "5     False\n",
      "6     False\n",
      "7     False\n",
      "8     False\n",
      "9     False\n",
      "10    False\n",
      "11    False\n",
      "12    False\n",
      "13    False\n",
      "14    False\n",
      "15    False\n",
      "16    False\n",
      "17    False\n",
      "18    False\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201955.0</td>\n",
       "      <td>2869717.0</td>\n",
       "      <td>4484442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4175000.0</td>\n",
       "      <td>-126027.0</td>\n",
       "      <td>-3081055.0</td>\n",
       "      <td>1729541.0</td>\n",
       "      <td>13868.0</td>\n",
       "      <td>1729541.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>304805.0</td>\n",
       "      <td>126027.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2902.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>2195.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>178980.0</td>\n",
       "      <td>182466.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>257817.0</td>\n",
       "      <td>3486.0</td>\n",
       "      <td>257817.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>477.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>916197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-560222.0</td>\n",
       "      <td>-5104.0</td>\n",
       "      <td>5243487.0</td>\n",
       "      <td>56301.0</td>\n",
       "      <td>4046157.0</td>\n",
       "      <td>864523.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1757552.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>267102.0</td>\n",
       "      <td>1295738.0</td>\n",
       "      <td>5634343.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1386055.0</td>\n",
       "      <td>10623258.0</td>\n",
       "      <td>11200.0</td>\n",
       "      <td>6680544.0</td>\n",
       "      <td>2660303.0</td>\n",
       "      <td>1586055.0</td>\n",
       "      <td>3942714.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>239671.0</td>\n",
       "      <td>260455.0</td>\n",
       "      <td>827696.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>-82782.0</td>\n",
       "      <td>-201641.0</td>\n",
       "      <td>63014.0</td>\n",
       "      <td>129142.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145796.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2    3          4         5          6   \\\n",
       "0  201955.0  2869717.0  4484442.0  0.0  4175000.0 -126027.0 -3081055.0   \n",
       "1       0.0   178980.0   182466.0  0.0        0.0       0.0        0.0   \n",
       "2     477.0        0.0   916197.0  0.0        0.0 -560222.0    -5104.0   \n",
       "3  267102.0  1295738.0  5634343.0  0.0  1200000.0       0.0 -1386055.0   \n",
       "4  239671.0   260455.0   827696.0  0.0   400000.0  -82782.0  -201641.0   \n",
       "\n",
       "           7         8          9          10         11         12   13  \\\n",
       "0   1729541.0   13868.0  1729541.0      152.0   304805.0   126027.0  0.0   \n",
       "1    257817.0    3486.0   257817.0        0.0        0.0        0.0  0.0   \n",
       "2   5243487.0   56301.0  4046157.0   864523.0        0.0  1757552.0  0.0   \n",
       "3  10623258.0   11200.0  6680544.0  2660303.0  1586055.0  3942714.0  0.0   \n",
       "4     63014.0  129142.0        0.0       69.0        0.0   145796.0  0.0   \n",
       "\n",
       "       14    15      16    17      18  \n",
       "0  2902.0  47.0  2195.0  65.0  1407.0  \n",
       "1     0.0   0.0     0.0   0.0     0.0  \n",
       "2   566.0  39.0    29.0   0.0   465.0  \n",
       "3     0.0   0.0     0.0   0.0     0.0  \n",
       "4     0.0   0.0     0.0   0.0     0.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming features into a df so that I won't have to remember to transform both features train and test.\n",
    "df_features = pd.DataFrame(features)\n",
    "print \"Any null-values present in the features chosen? \\n\", df_features.isnull().any()\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into train and test for features and labels\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(df_features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importances using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Ranking: \n",
      "  1.  feature: salary (0.220426513942)\n",
      "  2.  feature: deferral_payments (0.21197488041)\n",
      "  3.  feature: total_payments (0.132625994695)\n",
      "  4.  feature: loan_advances (0.106100795756)\n",
      "  5.  feature: bonus (0.105863661155)\n",
      "  6.  feature: restricted_stock_deferred (0.0757862826828)\n",
      "  7.  feature: deferred_income (0.0736811081639)\n",
      "  8.  feature: total_stock_value (0.0620731020005)\n",
      "  9.  feature: expenses (0.0114676611954)\n",
      "  10.  feature: exercised_stock_options (0.0)\n",
      "  11.  feature: other (0.0)\n",
      "  12.  feature: long_term_incentive (0.0)\n",
      "  13.  feature: restricted_stock (0.0)\n",
      "  14.  feature: director_fees (0.0)\n",
      "  15.  feature: to_messages (0.0)\n",
      "  16.  feature: from_poi_to_this_person (0.0)\n",
      "  17.  feature: from_messages (0.0)\n",
      "  18.  feature: from_this_person_to_poi (0.0)\n",
      "  19.  feature: shared_receipt_with_poi (0.0)\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# DecisionTree #\n",
    "################\n",
    "\n",
    "# Fitting the model\n",
    "clf = tree.DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# Feature Importances to identify which features have a high variance to be included in final model and which to exclude.\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print \"Feature Ranking: \"\n",
    "for i in range(len(importances)):\n",
    "    print \"  {}.  feature: {} ({})\".format(i+1, features_list[i+1], importances[indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajawad\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\ajawad\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ajawad\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\ajawad\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>bonus_salary_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>314288.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1101393.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-41250.0</td>\n",
       "      <td>495633.0</td>\n",
       "      <td>27861.0</td>\n",
       "      <td>117551.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>378082.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>182245.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2692324.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1008941.0</td>\n",
       "      <td>21530.0</td>\n",
       "      <td>601438.0</td>\n",
       "      <td>53775.0</td>\n",
       "      <td>2234774.0</td>\n",
       "      <td>407503.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>758931.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>664375.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94556.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1433.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6077885.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5127155.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>950730.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-472568.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>662086.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.797693e+308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0    1          2    3         4         5        6          7  \\\n",
       "28  314288.0  0.0  1101393.0  0.0  800000.0       0.0 -41250.0   495633.0   \n",
       "40  182245.0  0.0  2692324.0  0.0  200000.0       0.0      0.0  1008941.0   \n",
       "86       0.0  0.0        0.0  0.0       0.0       0.0      0.0   758931.0   \n",
       "24       0.0  0.0        0.0  0.0       0.0       0.0      0.0  6077885.0   \n",
       "25       0.0  0.0        0.0  0.0       0.0 -472568.0      0.0   189518.0   \n",
       "\n",
       "          8          9       10         11        12   13      14    15  \\\n",
       "28  27861.0   117551.0    494.0        0.0  378082.0  0.0   102.0   0.0   \n",
       "40  21530.0   601438.0  53775.0  2234774.0  407503.0  0.0     0.0   0.0   \n",
       "86      0.0   664375.0      0.0        0.0   94556.0  0.0  1433.0  25.0   \n",
       "24      0.0  5127155.0      0.0        0.0  950730.0  0.0     0.0   0.0   \n",
       "25      0.0        0.0      0.0        0.0  662086.0  0.0     0.0   0.0   \n",
       "\n",
       "       16   17     18  bonus_salary_ratio  \n",
       "28   33.0  4.0   71.0        0.000000e+00  \n",
       "40    0.0  0.0    0.0        0.000000e+00  \n",
       "86  215.0  2.0  508.0        0.000000e+00  \n",
       "24    0.0  0.0    0.0        0.000000e+00  \n",
       "25    0.0  0.0    0.0      -1.797693e+308  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New feature is bonus/salary and null-values replaced by 0\n",
    "features_train['bonus_salary_ratio'] = features_train.loc[:, 5] / features_train.loc[:, 1]\n",
    "features_train['bonus_salary_ratio'] = np.nan_to_num(features_train['bonus_salary_ratio'])\n",
    "\n",
    "# Repeat same feature engineering for test data\n",
    "features_test['bonus_salary_ratio'] = features_test.loc[:, 5] / features_test.loc[:, 1]\n",
    "features_test['bonus_salary_ratio'] = np.nan_to_num(features_test['bonus_salary_ratio'])\n",
    "\n",
    "features_train.head()\n",
    "\n",
    "# New feature proved to make my models perform worse. See explanation at the Q&A section at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Ranking: \n",
      "  1.  feature: salary (0.220426513942)\n",
      "  2.  feature: deferral_payments (0.21197488041)\n",
      "  3.  feature: total_payments (0.132625994695)\n",
      "  4.  feature: loan_advances (0.106100795756)\n",
      "  5.  feature: bonus (0.105863661155)\n",
      "  6.  feature: restricted_stock_deferred (0.0757862826828)\n",
      "  7.  feature: deferred_income (0.0736811081639)\n",
      "  8.  feature: total_stock_value (0.0620731020005)\n",
      "  9.  feature: expenses (0.0114676611954)\n",
      "  10.  feature: bonus_salary_ratio (0.0)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-50245f2aae25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Feature Ranking: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"  {}.  feature: {} ({})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimportances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Refining features_list to only include features with >0 importance \n",
    "#and to include the new feature in features_list\n",
    "\n",
    "features_list = ['poi', 'salary', 'deferral_payments', 'total_payments', \n",
    "                'loan_advances', 'bonus', 'restricted_stock_deferred',\n",
    "                'deferred_income', 'total_stock_value', 'expenses', 'bonus_salary_ratio'\n",
    "               ]\n",
    "\n",
    "# Feature Importances to see whether new feature created has any importance.\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print \"Feature Ranking: \"\n",
    "for i in range(len(importances)):\n",
    "    print \"  {}.  feature: {} ({})\".format(i+1, features_list[i+1], importances[indices[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing new feature due to zero importance\n",
    "features_list = ['poi', 'salary', 'deferral_payments', 'total_payments', \n",
    "                'loan_advances', 'bonus', 'restricted_stock_deferred',\n",
    "                'deferred_income', 'total_stock_value', 'expenses'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling using MinMaxScaling since numerical values span positive and negative\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Slicing all rows but ignoring first column since it's a bool (\"poi\")\n",
    "features_train_scaled = min_max_scaler.fit_transform(features_train.iloc[:, 1:])\n",
    "features_test_scaled = min_max_scaler.fit_transform(features_test.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.863636363636\n",
      "Precision score:  0.4\n",
      "Recall score:  0.4\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# AdaBoost #\n",
    "############\n",
    "\n",
    "clf = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# params = {\n",
    "#    'n_estimators':np.arange(1, 200, 1),\n",
    "#    'learning_rate':np.linspace(0.1, 0.00001, num=1000),\n",
    "#    'algorithm':['SAMME', 'SAMME.R']\n",
    "# }\n",
    "\n",
    "# clf_ada_rscv = RandomizedSearchCV(clf, param_distributions=params,n_iter=100, cv=80, scoring='recall', n_jobs=-1, verbose=2)\n",
    "# clf_ada_rscv.fit(features_train_scaled, labels_train)\n",
    "\n",
    "# time.sleep(2)\n",
    "# print clf_ada_rscv.best_params_\n",
    "# print clf_ada_rscv.best_score_\n",
    "\n",
    "# {'n_estimators': 173, 'learning_rate': 0.043248918918918917, 'algorithm': 'SAMME.R'}\n",
    "# 0.1\n",
    "\n",
    "clf.fit(features_train_scaled, labels_train)\n",
    "pred = clf.predict(features_test_scaled)\n",
    "\n",
    "# Classifier scores\n",
    "precision_score_ada = precision_score(pred, labels_test)\n",
    "recall_score_ada = recall_score(pred, labels_test)\n",
    "accuracy_score_ada = accuracy_score(pred, labels_test)\n",
    "\n",
    "print \"Accuracy score: \", accuracy_score_ada\n",
    "print \"Precision score: \", precision_score_ada\n",
    "print \"Recall score: \", recall_score_ada\n",
    "\n",
    "# Dumping classifier, my_dataset and features_list as .pkl files to be used in tester.py\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "# tester.py scores: Precision: 0.40000      Recall: 0.30300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.613636363636\n",
      "Precision score:  0.6\n",
      "Recall score:  0.166666666667\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# KNearest Neighbour #\n",
    "######################\n",
    "\n",
    "clf = KNeighborsClassifier(p=1, weights='distance', leaf_size=19, algorithm='ball_tree', n_neighbors=2)\n",
    "\n",
    "# clf_tree_rscv = RandomizedSearchCV(clf, param_distributions=params, cv=70, n_iter=100, scoring='recall', n_jobs=-1, verbose=2)\n",
    "# clf_tree_rscv.fit(features_train, labels_train)\n",
    "\n",
    "# print clf_tree_rscv.best_params_\n",
    "# print clf_tree_rscv.best_score_\n",
    "\n",
    "# {'p': 1, 'weights': 'distance', 'leaf_size': 19, 'algorithm': 'ball_tree', 'n_neighbors': 2}\n",
    "# 0.09\n",
    "\n",
    "clf.fit(features_train_scaled, labels_train)\n",
    "pred = clf.predict(features_test_scaled)\n",
    "\n",
    "# Classifier scores\n",
    "precision_score_knn = precision_score(pred, labels_test)\n",
    "recall_score_knn = recall_score(pred, labels_test)\n",
    "accuracy_score_knn = accuracy_score(pred, labels_test)\n",
    "\n",
    "print \"Accuracy score: \", accuracy_score_tree\n",
    "print \"Precision score: \", precision_score_tree\n",
    "print \"Recall score: \", recall_score_tree\n",
    "\n",
    "# Dumping classifier, my_dataset and features_list as .pkl files to be used in tester.py\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.613636363636\n",
      "Precision score:  0.6\n",
      "Recall score:  0.166666666667\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Decision Tree #\n",
    "#################\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(random_state=42, criterion='gini', max_depth=17, max_features=18, class_weight='balanced', splitter='random', min_samples_leaf=14, min_samples_split=63)\n",
    "\n",
    "# clf_tree_rscv = RandomizedSearchCV(clf, param_distributions=parameters, cv=70, n_iter=80, scoring='precision', n_jobs=-1, verbose=2)\n",
    "# clf_tree_rscv.fit(features_train, labels_train)\n",
    "\n",
    "# print clf_tree_rscv.best_params_\n",
    "# print clf_tree_rscv.best_score_\n",
    "\n",
    "# {'splitter': 'random', 'min_samples_leaf': 14, 'max_features': 18, 'criterion': 'gini', 'min_samples_split': 49, 'max_depth': 17, 'class_weight': 'balanced'}\n",
    "# 0.25\n",
    "\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "# Classifier scores\n",
    "precision_score_tree = precision_score(pred, labels_test)\n",
    "recall_score_tree = recall_score(pred, labels_test)\n",
    "accuracy_score_tree = accuracy_score(pred, labels_test)\n",
    "\n",
    "print \"Accuracy score: \", accuracy_score_tree\n",
    "print \"Precision score: \", precision_score_tree\n",
    "print \"Recall score: \", recall_score_tree\n",
    "\n",
    "# Dumping classifier, my_dataset and features_list as .pkl files to be used in tester.py\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.886363636364\n",
      "Precision score:  0.0\n",
      "Recall score:  0.0\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# Naive Bayes #\n",
    "###############\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "# Classifier scores\n",
    "precision_score_gnb = precision_score(pred, labels_test)\n",
    "recall_score_gnb = recall_score(pred, labels_test)\n",
    "accuracy_score_gnb = accuracy_score(pred, labels_test)\n",
    "\n",
    "print \"Accuracy score: \", accuracy_score_gnb\n",
    "print \"Precision score: \", precision_score_gnb\n",
    "print \"Recall score: \", recall_score_gnb\n",
    "\n",
    "# Dumping classifier, my_dataset and features_list as .pkl files to be used in tester.py\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.886363636364\n",
      "Precision score:  0.4\n",
      "Recall score:  0.5\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "# GBM #\n",
    "#######\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# parameters = {'learning_rate':np.linspace(0.1, 0.00001, num=1000),\n",
    "#              'n_estimators':np.arange(1, 100, 1),\n",
    "#              'max_depth':np.arange(1, 13, 1),\n",
    "#              'max_features':np.arange(1, 20, 1),\n",
    "#              'min_samples_split':np.arange(2, 20, 1),\n",
    "#              'min_samples_leaf':np.arange(1, 20, 1)\n",
    "#             }\n",
    "# clf_gbrt_rscv = RandomizedSearchCV(clf_gbrt, param_distributions=parameters, cv=70, n_iter=100, scoring='precision', n_jobs=-1, verbose=2)\n",
    "# clf_gbrt_rscv.fit(features_train_scaled, labels_train)\n",
    "\n",
    "# print clf_gbrt_rscv.best_params_\n",
    "# print clf_gbrt_rscv.best_score_\n",
    "\n",
    "# {'learning_rate': 0.042548288288288286, 'min_samples_leaf': 1, 'n_estimators': 99, 'max_features': 3, 'min_samples_split': 10, 'max_depth': 1}\n",
    "# 0.06\n",
    "\n",
    "clf.fit(features_train_scaled, labels_train)\n",
    "pred = clf.predict(features_test_scaled)\n",
    "\n",
    "# Classifier scores\n",
    "precision_score_gbm = precision_score(pred, labels_test)\n",
    "recall_score_gbm = recall_score(pred, labels_test)\n",
    "accuracy_score_gbm = accuracy_score(pred, labels_test)\n",
    "\n",
    "print \"Accuracy score: \", accuracy_score_gbm\n",
    "print \"Precision score: \", precision_score_gbm\n",
    "print \"Recall score: \", recall_score_gbm\n",
    "\n",
    "# Dumping classifier, my_dataset and features_list as .pkl files to be used in tester.py\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.795454545455\n",
      "Precision score:  0.2\n",
      "Recall score:  0.166666666667\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "# SVC #\n",
    "#######\n",
    "\n",
    "clf = SVC(random_state=42, kernel='sigmoid', C=57.9, gamma=0.059, class_weight='balanced')\n",
    "# params = {'C': scipy.stats.expon(scale=100), \n",
    "#          'gamma': scipy.stats.expon(scale=.1), \n",
    "#          'kernel':['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "#          'class_weight': [None, 'balanced']\n",
    "#         }\n",
    "# clf_svc_rscv = RandomizedSearchCV(clf, param_distributions=params, cv=70, n_iter=50, scoring='precision', verbose=2, n_jobs=-1)\n",
    "# clf_svc_rscv.fit(features_train_scaled, labels_train)\n",
    "clf.fit(features_train_scaled, labels_train)\n",
    "\n",
    "pred = clf.predict(features_test_scaled)\n",
    "\n",
    "# Classifier scores\n",
    "precision_score_svc = precision_score(pred, labels_test)\n",
    "recall_score_svc = recall_score(pred, labels_test)\n",
    "accuracy_score_svc = accuracy_score(pred, labels_test)\n",
    "\n",
    "print \"Accuracy score: \", accuracy_score_svc\n",
    "print \"Precision score: \", precision_score_svc\n",
    "print \"Recall score: \", recall_score_svc\n",
    "\n",
    "# print clf_svc_rscv.best_params_\n",
    "# print clf_svc_rscv.best_score_\n",
    "\n",
    "# parameters that return the best recall score:\n",
    "# {'kernel': 'sigmoid', 'C': 22.124794209078111, 'gamma': 0.38710403019966072, 'class_weight': 'balanced'}\n",
    "# 0.33\n",
    "\n",
    "# parameters that return the best precision score:\n",
    "# {'kernel': 'sigmoid', 'C': 57.914218875973347, 'gamma': 0.059626099388516422, 'class_weight': 'balanced'}\n",
    "# 0.235\n",
    "\n",
    "# Dumping classifier, my_dataset and features_list as .pkl files to be used in tester.py\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1, 'min_samples_split': 5, 'max_depth': 34, 'min_samples_leaf': 1}\n",
      "0.21\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Random Forest #\n",
    "#################\n",
    "\n",
    "# Fitting the model\n",
    "clf = RandomForestClassifier(random_state=42, n_estimators=1, min_samples_leaf=2, min_samples_split=2, max_depth=31)\n",
    "\n",
    "# RandomizedsearchCV to find optimal hyper params\n",
    "# params = {\n",
    "#          'n_estimators':np.arange(1, 5, 1),\n",
    "#          'min_samples_leaf':np.arange(1, 5, 1),\n",
    "#          'min_samples_split':np.arange(2, 20, 1),\n",
    "#          'max_depth':np.arange(1, 40, 1),\n",
    "#         }\n",
    "\n",
    "# clf_rf_rscv = RandomizedSearchCV(clf, cv=70, n_iter=50, param_distributions=params, scoring='recall', verbose=2, n_jobs=-1)\n",
    "# clf_rf_rscv.fit(features_train, labels_train)\n",
    "\n",
    "# print clf_rf_rscv.best_params_\n",
    "# print clf_rf_rscv.best_score_\n",
    "\n",
    "# {'n_estimators': 1, 'min_samples_split': 2, 'max_depth': 31, 'min_samples_leaf': 2}\n",
    "# 0.21\n",
    "\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dumping classifier, my_dataset and features_list as .pkl files to be used in tester.py\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    ">Machine learning is powerful at predicting whether a certain outcome is likely to happen (classification) or continuous numbers (regression). In this example where we are asked to predict that a person is a POI (classification) and we have features such as salary, bonus, stock (financial) as well as how many emails they have sent/received (count) we can use such features to learn if these help us predict whether people are POIs or not. As for outliers, I have only removed one due to the scarcity of data to begin with. The one I have removed is the column sum in the PDF of Enron employees' salaries (employee name \"TOTAL\") because that's not a feature of an employee.\n",
    "\n",
    "> There were 146 rows of data pre-cleaning and 18 POIs and 126 Non-POIs in the dataset.\n",
    "\n",
    "2.What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n",
    "\n",
    ">As part of the EDA I looked at feature importances to understand which features were important and which weren't useful to include in my machine learning algorithms. I took all features which had an importance > 0, so that means the following 9 features: 'poi', 'salary', 'deferral_payments', 'total_payments','loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses'. As for the feature importances scores for the features I have used they are as follows:   \n",
    "  1. feature: salary (0.220426513942)\n",
    "  2. feature: deferral_payments (0.21197488041)\n",
    "  3. feature: total_payments (0.132625994695)\n",
    "  4. feature: loan_advances (0.106100795756)\n",
    "  5. feature: bonus (0.105863661155)\n",
    "  6. feature: restricted_stock_deferred (0.0757862826828)\n",
    "  7. feature: deferred_income (0.0736811081639)\n",
    "  8. feature: total_stock_value (0.0620731020005)\n",
    "  9. feature: expenses (0.0114676611954)\n",
    "\n",
    "> I have scaled the numerical features in my SVC model using MinMaxScaler to easier be able to compare the different financial measurements on a scale of 0 to 1.\n",
    "> I have created my own feature bonus_salary_ratio which is bonus / salary based on the rationale that someone who has a high salary (top correlated feature with a POI) is likely also to have a high bonus however it made my models perform worse, especially GaussianNB due to the large amount of bonuses AND salaries which both were 0's so I was dividing 0 by 0, or some bonuses being negative leading to a -inf when divided by salary.\n",
    "\n",
    ">As mentioned earlier I have decided to include any feature with a score > 0.\n",
    "\n",
    "3.What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "\n",
    "> I ended it up using AdaBoost with default parameters but with features scaled using the MinMaxScaler because it returned a precision of .4 and recall of .3 which was the highest scores out of them all. I tried SVC, GBM, KNN, DecisionTree, GaussianNB and Random Forest but none returned scores above .3 for both precision and recall. Even when I fine-tuned the parameters using RanomizedSearchCV for all of the above models it was still the default parameters for AdaBoost that performed the best on tester.py.\n",
    "\n",
    "4.What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "\n",
    "> Tuning the parameters of an algorithm is really customising the model to match your dataset. For some of the parameters such as min_samples_split there is a trade-off between performance and accuracy: a high sample_split means you are avoiding over-fitting whereas a too high value means you are underfitting. Likewise with the learning_rate, if you give it too high a value you risk it might miss the optimal point whereas if it's too low it will take too long to converge and reach the local minima. For Random Forests, I've tuned the main parameters using RandomizedSearchCV but the best model didn't make it to the .3 precisio/recall scores. Below were the params and their ranges which I used for RandomizedSearchCV for Random Forest:\n",
    "\n",
    ">params = {\n",
    "          'n_estimators':np.arange(1, 5, 1),\n",
    "          'min_samples_leaf':np.arange(1, 5, 1),\n",
    "          'min_samples_split':np.arange(2, 20, 1),\n",
    "          'max_depth':np.arange(1, 40, 1),\n",
    "         }\n",
    "         \n",
    "> and for SVC I have used the following params:\n",
    "\n",
    "> params = {'C': scipy.stats.expon(scale=100), \n",
    "          'gamma': scipy.stats.expon(scale=.1), \n",
    "          'kernel':['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "          'class_weight': [None, 'balanced']\n",
    "         }\n",
    "> if my AdaBoost classifier with default parameters weren't the top performing model I would have fine-tuned my model using RandomizedSearchCV like I did for Random Forest and SVC to find the optimal parameters to use in my model.\n",
    "\n",
    "5.What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]\n",
    "\n",
    "> Validation, also known as cross-validation. It is used to prevent over-fitting to the training data. If you don't use cross-validation you risk overfitting your model to the training dataset which means it won't be able to perform well on un-seen new data because it is unable to generalise well. CV works by splitting the training dataset into smaller sets which the model is evaluated on and for each fold it will then return the score average accuracy score from all of the folds. CV is especially important with imbalanced classes as it increases the probability that your CV folds are more representative of the data.\n",
    "\n",
    "6.Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "> As mentioned above my best performing model was an AdaBoost with default parameters. When I ran the tester.py I got Precision score of 0.40000 and a Recall score 0.30300. This means that in 40% of the predictions my model were accurate in predicting the true positives out of the sum of both true positives and false positives. Similarly, for the Recall score it was able to correctly identify all positive samples 30.3% of the time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
